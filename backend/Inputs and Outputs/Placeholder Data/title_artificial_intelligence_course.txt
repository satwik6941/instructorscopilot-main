Fundamentals of Artificial Intelligence: Hands-On AI Projects

## Course Overview
This course provides intermediate learners with a hands-on, project-based understanding of core Artificial Intelligence concepts. It progresses from the philosophical and architectural foundations of AI agents through classic problem-solving techniques, culminating in logic, planning, and knowledge representation. Each module emphasizes practical application through coding exercises and culminating projects, fostering a clear and structured learning path, enabling students to design and implement various intelligent systems in Python.

## Weekly Summary
*   **Week 1:** Explore the foundational concepts of AI, its history, philosophical debates, and the core idea of intelligent agents and their environments.
*   **Week 2:** Delve into systematic problem-solving by implementing and evaluating various uninformed and informed search algorithms, notably A*.
*   **Week 3:** Learn about adversarial search techniques, including the Minimax algorithm and Alpha-Beta Pruning, to develop AI for competitive games.
*   **Week 4:** Discover how to formulate and solve problems using Constraint Satisfaction, alongside an introduction to formal logic for knowledge representation and inference.
*   **Week 5:** Focus on automated planning, designing sequences of actions for intelligent agents, and explore the basics of multi-agent systems.
*   **Week 6:** Deepen understanding of knowledge representation and reasoning, including ontological engineering, and apply these concepts to build simple knowledge-based systems.

# Week 1: Foundations of AI and Intelligent Agents

Concepts:
*   **Defining AI:** Artificial Intelligence can be viewed through different lenses: thinking humanly (cognitive modeling), acting humanly (Turing Test), thinking rationally (laws of thought), and acting rationally (maximizing expected outcomes). This course primarily focuses on "acting rationally," aiming to build systems that make optimal decisions.
*   **Brief History of AI:** The field has seen cycles of excitement and "AI winters." Early milestones include the Logic Theorist (1950s) and expert systems (1960s-80s). Modern resurgence (2000s-Present) is driven by big data, increased computational power, and advances in machine learning, especially deep learning.
*   **Intelligent Agents:** An intelligent agent is an autonomous entity that perceives its environment through sensors and acts upon that environment through actuators. Rationality is key: an agent is rational if it chooses actions that maximize its expected performance measure given its percept sequence (history of perceptions).
*   **Agent Architectures:** Agents can be structured in various ways: Simple Reflex Agents (action based solely on current percept), Model-Based Reflex Agents (maintain an internal state or "model" of the world), Goal-Based Agents (use goals to guide decision-making), and Utility-Based Agents (maximize a utility function representing desirable states).
*   **Environments (PEAS):** The PEAS (Performance, Environment, Actuators, Sensors) framework helps characterize an agent's task environment. Understanding environment properties is crucial: fully vs. partially observable, deterministic vs. stochastic, episodic vs. sequential, static vs. dynamic, discrete vs. continuous, single-agent vs. multi-agent.

Example:
Consider a simple "Roomba" vacuum cleaner agent.
*   **Performance:** Maximize cleanliness, minimize cleaning time, minimize noise, avoid damage.
*   **Environment:** A house with rooms, furniture, dirt, and potentially obstacles.
*   **Actuators:** Wheels for movement, vacuum for sucking dirt.
*   **Sensors:** Dirt sensor, bump sensor, location sensor.
*   If designed as a simple reflex agent, it might have a rule: `If dirt detected, then suck; else, move forward`. A more complex agent would build a map (internal model) of the house and plan a cleaning path.

Case Study:
Self-driving cars represent a complex intelligent agent system. They continuously perceive their environment using a suite of sensors (cameras, radar, lidar), build an internal model of the world (other cars, pedestrians, traffic signs, road conditions), set goals (reach destination, obey traffic laws, minimize travel time), and execute actions (steer, accelerate, brake) through actuators. The stochastic and dynamic nature of road environments makes their decision-making highly challenging, requiring sophisticated utility-based reasoning to balance safety and efficiency.

Exercise:
Choose two common AI applications (e.g., a smart thermostat, an online recommender system, a chess AI, a chatbot). For each, identify its PEAS description and describe the relevant properties of its operating environment (e.g., fully observable, stochastic, sequential).

Tip/Pitfall:
A common pitfall is confusing "thinking humanly" with "acting rationally." While humans might not always act perfectly rationally due to biases or limited information, a rational AI agent aims for optimal outcomes given the available information, even if its internal thought processes are completely unlike a human's. Focus on the rational agent paradigm for building effective AI systems.

# Week 2: Problem Solving with Search Algorithms

Concepts:
*   **Problem Formulation:** To solve a problem using search, we first formulate it by defining: the set of possible states, the actions (or operators) that transition between states, the initial state where the agent begins, a goal test to determine if a state is a solution, and a path cost function to evaluate the expense of a sequence of actions.
*   **Graph Representation:** Search problems are often visualized as graphs where nodes represent states and edges represent actions. Adjacency lists (storing a list of neighbors for each node) and adjacency matrices (a matrix indicating direct connections) are common ways to represent these graphs in code.
*   **Uninformed Search Strategies:** These algorithms explore the search space without any domain-specific knowledge or heuristics about the goal. Key examples include Breadth-First Search (BFS), which explores layer by layer, guaranteeing optimality for uniform step costs, and Depth-First Search (DFS), which explores as deep as possible before backtracking. Iterative Deepening DFS combines the benefits of both.
*   **Informed Search Strategies:** Unlike uninformed methods, these strategies use a heuristic function, an estimate of the cost from any current state to the goal state, to guide the search. While Greedy Best-First Search uses only the heuristic, it doesn't guarantee optimality.
*   **A* Search Algorithm:** This is a widely used informed search algorithm that combines the cost from the start node to the current node (denoted g(n)) and the estimated cost from the current node to the goal (denoted h(n)). It always expands the node with the lowest total estimated cost, f(n) = g(n) + h(n). A* is optimal and complete if its heuristic function is admissible (never overestimates the cost to the goal) and consistent (monotonic).

Example:
Consider finding the shortest path through a city grid with obstacles, like a simple maze.
*   **States:** Each cell (x, y) in the grid.
*   **Actions:** Move Up, Down, Left, Right (if not blocked by an obstacle).
*   **Initial State:** Starting cell (0,0).
*   **Goal Test:** Current cell is the destination cell (e.g., 9,9).
*   **Path Cost:** Each move costs 1 unit.
*   For A* search, a suitable heuristic h(n) could be the Manhattan distance (sum of absolute differences in x and y coordinates) from the current cell to the goal cell, as it never overestimates the true path cost in a grid.

Case Study:
GPS navigation systems heavily rely on pathfinding algorithms. When you request directions, the system formulates the problem as finding the shortest path through a graph of roads (states) and intersections (nodes). It considers current traffic (path costs) and uses efficient algorithms like A* search, often with sophisticated heuristics (like estimated travel time based on historical data) to quickly find an optimal or near-optimal route across a vast network. The efficiency and optimality of these algorithms directly impact user experience.

Exercise:
Given a simple 5x5 grid maze with a start, an end, and 3-4 obstacles:
1.  Manually trace the path a Breadth-First Search would explore to find the shortest path.
2.  Then, consider how an A* search, using Manhattan distance as its heuristic, would navigate the same maze. Note which nodes it might explore first compared to BFS.

Tip/Pitfall:
A common pitfall in A* search is designing a heuristic function that is not admissible or consistent. An inadmissible heuristic can lead the algorithm astray, causing it to find sub-optimal paths or, in extreme cases, fail to find a path at all. Always ensure your heuristic provides an *underestimate* of the true cost to the goal.

# Week 3: Adversarial Search and Game Playing AI

Concepts:
*   **Game Theory Basics:** Adversarial search focuses on multi-agent environments where agents compete, often in "perfect information" games (all players know the full game state) and "zero-sum" games (one player's gain is another's equivalent loss). Examples include Chess, Tic-Tac-Toe, and Connect Four.
*   **Game Tree Representation:** Games can be modeled using game trees, where nodes represent game states, and edges represent possible moves. The root is the current state, and branches extend to future possible states. Terminal nodes are end-game states (win, loss, or draw).
*   **Minimax Algorithm:** This is a recursive algorithm used by the maximizing player (who wants to maximize their score) to choose the optimal move assuming the opponent (minimizing player) will also play optimally to minimize the maximizing player's score. It works by assigning values to terminal nodes and then "backing up" these values through the tree to determine the best move at each non-terminal state.
*   **Alpha-Beta Pruning:** An essential optimization for the Minimax algorithm that significantly reduces the number of nodes that need to be evaluated in the game tree. It achieves this by eliminating (pruning) branches that cannot possibly influence the final optimal decision. Alpha (α) represents the best score the maximizing player is currently guaranteed, and Beta (β) represents the best score the minimizing player is currently guaranteed. Pruning occurs when α is greater than or equal to β.
*   **Evaluation Functions:** For games with very large or infinite game trees (where it's impossible to search to terminal nodes), an evaluation function estimates the "goodness" of a non-terminal game state. This function assigns a numerical score to a game board configuration, guiding the search when full tree exploration is infeasible. Designing a good evaluation function is often crucial for strong game AI.

Example:
Consider a simple Tic-Tac-Toe game.
*   The game state can be represented as a 3x3 array.
*   The Minimax algorithm would recursively explore all possible moves from the current state. For the AI's turn (maximizing player), it would choose the move that leads to the highest possible score. For the opponent's turn (minimizing player), it would assume the opponent makes the move that leads to the lowest score for the AI.
*   Alpha-Beta Pruning would then cut off branches where it's clear that a better move has already been found, either by the maximizing player (alpha cut-off) or by the minimizing player (beta cut-off). For instance, if the AI finds a winning move early, it doesn't need to explore other branches that lead to less favorable or equally winning outcomes.

Case Study:
IBM's Deep Blue, which defeated world chess champion Garry Kasparov in 1997, is a prime example of an AI system built on adversarial search. Deep Blue used a highly optimized Minimax algorithm combined with Alpha-Beta Pruning. Due to the immense size of the chess game tree, it couldn't explore every possibility. Instead, it relied heavily on sophisticated, hand-tuned evaluation functions to assess board positions and powerful parallel processing to search many ply (levels) deep within the time limits.

Exercise:
Take a simple game tree (e.g., a simplified Tic-Tac-Toe game after a few moves, or a game of Nim with small piles) and manually apply the Minimax algorithm. Then, apply Alpha-Beta Pruning to the same tree, explicitly marking which branches are pruned and why (i.e., identifying alpha/beta cut-offs).

Tip/Pitfall:
A common pitfall is misunderstanding the `alpha` and `beta` values in Alpha-Beta Pruning. Remember that `alpha` is the best score *for the maximizing player* found so far on the current path, and `beta` is the best score *for the minimizing player* found so far. Pruning happens when the current node's value cannot possibly be better than the opposing player's already guaranteed best move.

# Week 4: Constraint Satisfaction Problems and Logic-Based AI

Concepts:
*   **Constraint Satisfaction Problems (CSPs):** A CSP is a problem defined by a set of variables, each with a domain of possible values, and a set of constraints that restrict the values the variables can simultaneously take. Constraints can be unary (affecting one variable), binary (relating two variables), or higher-order.
*   **Formulating CSPs:** Many real-world problems can be modeled as CSPs, such as N-Queens (placing N queens on a chessboard without attacking each other), Sudoku (filling a grid with numbers respecting row, column, and block uniqueness), and Map Coloring (assigning colors to regions such that adjacent regions have different colors).
*   **Backtracking Search for CSPs:** This is the most common algorithm for solving CSPs. It's a recursive depth-first search that incrementally assigns values to variables. If an assignment violates a constraint, it "backtracks" to the last valid decision point and tries a different value.
*   **Heuristics for CSPs:** To improve the efficiency of backtracking search, heuristics are used: Minimum Remaining Values (MRV) chooses the variable with the fewest legal values (most constrained variable), while Least Constraining Value (LCV) selects the value that rules out the fewest choices for neighboring variables.
*   **Constraint Propagation:** Techniques like Forward Checking and Arc Consistency (AC-3) are used to prune the domains of unassigned variables early, before actual assignments are made. Forward checking removes inconsistent values from the domains of neighboring variables immediately after an assignment. Arc consistency ensures that for every value in a variable's domain, there is a consistent value in the domain of any neighboring variable.
*   **Propositional Logic:** A foundational form of logic for knowledge representation where statements are atomic (either true or false) and combined using logical connectives (AND, OR, NOT, IMPLIES, BICONDITIONAL). Inference can be performed using truth tables or resolution, converting sentences to Conjunctive Normal Form (CNF) for systematic proof.
*   **First-Order Logic (FOL):** An extension of propositional logic that allows for representing objects, properties, and relationships using constants, predicates, functions, and quantifiers (universal ∀ and existential ∃). FOL is more expressive, enabling complex knowledge bases and reasoning processes like unification, forward chaining (data-driven inference), and backward chaining (goal-driven inference).

Example:
Solving a Sudoku puzzle as a CSP:
*   **Variables:** Each empty cell in the 9x9 grid.
*   **Domain:** For each variable, the set of digits {1, 2, ..., 9}.
*   **Constraints:**
    *   All cells in a row must have unique values.
    *   All cells in a column must have unique values.
    *   All cells in each 3x3 subgrid must have unique values.
A backtracking search algorithm would iteratively pick an empty cell (variable), try assigning a value from its domain, and then check if this assignment violates any Sudoku rules. If it does, it tries another value; otherwise, it moves to the next empty cell. Heuristics like MRV would suggest filling cells that have fewer possible choices first.

Case Study:
University timetabling and resource allocation problems are often solved using CSPs. For instance, assigning classes to classrooms and time slots involves variables (e.g., specific class sessions), domains (e.g., available rooms, time slots), and numerous constraints (e.g., a room can only be used by one class at a time, a professor cannot teach two classes simultaneously, classes requiring specific equipment must be in equipped rooms). CSP solvers can efficiently find schedules that satisfy all these complex requirements, avoiding conflicts and optimizing resource usage.

Exercise:
Consider the problem of coloring a map of four regions: A, B, C, D.
*   A is adjacent to B, C.
*   B is adjacent to A, D.
*   C is adjacent to A, D.
*   D is adjacent to B, C.
Assume you have three colors: Red, Green, Blue.
1.  Formulate this as a CSP: Define variables, their domains, and all binary constraints.
2.  Manually apply backtracking search to find a valid coloring. Show your steps.

Tip/Pitfall:
A common pitfall in CSPs is inefficient search due to poor variable/value ordering. Using heuristics like Minimum Remaining Values (MRV) to select the next variable and Least Constraining Value (LCV) to select the value can dramatically speed up the search by pruning the search space earlier, making the problem solvable in practice even for large instances.

# Week 5: AI Planning and Multi-Agent Systems

Concepts:
*   **Classical Planning:** Unlike search (which finds a path to a goal state), planning explicitly constructs a sequence of actions that transforms an initial state into a desired goal state. It involves reasoning about the preconditions (what must be true before an action can occur) and effects (what changes result from an action) of actions over time.
*   **STRIPS Framework:** The Stanford Research Institute Problem Solver (STRIPS) is a classic formalism for representing planning problems. States are described by a set of logical predicates, and actions are defined by their preconditions (literals that must be true), an add list (literals that become true after the action), and a delete list (literals that become false after the action).
*   **Planning Algorithms:**
    *   **Forward (Progression) Planning:** Starts from the initial state and applies actions to move towards the goal state. This is essentially a search in the state space.
    *   **Backward (Regression) Planning:** Starts from the goal state and works backward, finding actions that could have led to the current state, until the initial state is reached. This can be more efficient if the goal state is well-defined.
    *   **Planning Graphs:** A graphical structure used to quickly estimate whether a goal is reachable and to derive heuristics for planning algorithms. They show what propositions can be true and what actions are possible at each level of time, assuming parallel execution.
*   **Hierarchical Task Networks (HTN):** A more advanced planning approach that breaks down complex problems into smaller, more manageable subtasks. HTN planners use a set of "methods" to decompose tasks until they are primitive actions that can be executed.
*   **Planning in Complex Environments:** Real-world environments are often uncertain or nondeterministic. Conditional planning addresses this by considering multiple possible outcomes of an action. Replanning involves dynamically adjusting a plan during execution if unexpected events occur or initial assumptions change.
*   **Multi-Agent Systems (MAS):** These systems consist of multiple interacting intelligent agents that may cooperate, coordinate, or compete to achieve their individual or collective goals. Planning in MAS involves challenges like distributed planning (each agent plans independently) versus centralized planning (a single entity plans for all agents) and requires robust communication and negotiation mechanisms.

Example:
Consider a robot assembly task using STRIPS:
*   **Initial State:** `OnTable(A), OnTable(B), Clear(A), Clear(B), HandEmpty`
*   **Goal State:** `On(A, B), OnTable(B), HandEmpty`
*   **Actions:**
    *   `PickUp(x)`:
        *   Preconditions: `OnTable(x), Clear(x), HandEmpty`
        *   Add: `Holding(x)`
        *   Delete: `OnTable(x), Clear(x), HandEmpty`
    *   `PutOn(x, y)`:
        *   Preconditions: `Holding(x), Clear(y)`
        *   Add: `On(x, y), Clear(x)`
        *   Delete: `Holding(x), Clear(y)`
    *   A simple plan to achieve the goal could be: `PickUp(A)`, `PutOn(A, B)`. Each action changes the state based on its add and delete lists.

Case Study:
Automated warehouse management systems often use AI planning. Robots need to plan optimal paths to pick up and deliver items, navigating complex layouts and avoiding collisions. This involves not only single-agent planning for each robot but also multi-agent coordination to ensure robots don't block each other, manage shared resources (like charging stations), and efficiently fulfill orders in parallel. Replanning capabilities are vital to adapt to new orders, robot malfunctions, or unexpected obstacles.

Exercise:
Design a simple STRIPS-like problem for a "coffee robot" starting in the kitchen.
*   **Initial State:** Robot is in the kitchen, coffee machine is off, no coffee in cup.
*   **Goal State:** Coffee in cup, robot in living room.
*   Define at least three actions (e.g., `BrewCoffee`, `MoveTo(location)`, `PourCoffee`) with their preconditions and effects.

Tip/Pitfall:
A common pitfall is confusing planning with simple search. While planning *uses* search, its core distinction is the explicit representation of actions with preconditions and effects, which allows for reasoning about how sequences of actions change the world to achieve a goal, rather than just finding a path in a pre-defined state space.

# Week 6: Knowledge Representation and Reasoning

Concepts:
*   **The Role of Knowledge in AI:** Knowledge Representation (KR) is fundamental to intelligent systems, enabling them to store, organize, and reason about information. It allows AI to understand context, make inferences, and act intelligently, going beyond purely data-driven or reactive behaviors. Knowledge can be explicit (directly stored facts and rules) or implicit (derived from explicit knowledge or embedded in algorithms).
*   **Ontological Engineering:** This is the process of formally structuring knowledge within a specific domain. It involves defining categories (classes of objects), objects (instances), and relations (relationships between objects and categories). Common relations include "Is-A" (for hierarchies, e.g., Dog Is-A Mammal), "Has-A" (for properties, e.g., Dog Has-A Tail), and "Part-Of" (for components, e.g., Wheel Part-Of Car).
*   **Semantic Networks and Frames:** These are early formalisms for KR. Semantic networks represent knowledge as a graph with nodes (concepts/objects) and labeled edges (relations). Frames are data structures representing stereotypical objects or concepts, with slots for attributes and their values, including mechanisms for inheritance.
*   **Knowledge Graphs:** Modern knowledge bases that extend semantic networks, organizing information as a graph of interconnected entities and their relationships. They allow for complex querying and reasoning, enabling AI systems to infer new facts from existing ones.
*   **Reasoning Mechanisms:**
    *   **Inheritance:** Inferring properties of a specific object based on the properties of the category it belongs to (e.g., if "all birds can fly" and "sparrow is a bird," then "sparrow can fly").
    *   **Pattern Matching:** Identifying predefined patterns in the knowledge base to trigger actions or answer queries.
    *   **Forward Chaining:** A data-driven inference process that starts with known facts and applies rules to derive new conclusions.
    *   **Backward Chaining:** A goal-driven inference process that starts with a query (a goal) and works backward, trying to find facts and rules that support the query.
*   **Challenges in KR & Reasoning:**
    *   **Default Reasoning / Non-monotonic Logic:** Dealing with exceptions and beliefs that can change. Unlike classical logic, conclusions in non-monotonic logic can be retracted if new information arrives (e.g., "Birds fly, but penguins are birds and do not fly").
    *   **Closed World Assumption vs. Open World Assumption:** The closed-world assumption states that anything not known to be true is false. The open-world assumption states that anything not known to be true could be either true or false (reflecting incomplete knowledge).
    *   **Dealing with Uncertainty:** Introducing probabilistic reasoning (e.g., using Bayes' Theorem or Bayesian Networks conceptually) to handle situations where information is incomplete or unreliable, allowing AI to reason with probabilities rather than absolute true/false statements.

Example:
Building a simple knowledge base for animal classification in Python:
*   Facts: `IsA(dog, mammal)`, `IsA(cat, mammal)`, `HasProperty(mammal, fur)`, `Sound(dog, bark)`.
*   Rules (implicitly in code logic): If `IsA(X, Y)` and `HasProperty(Y, Z)`, then `HasProperty(X, Z)`.
*   Query: "Does a dog have fur?"
*   Reasoning:
    1.  Look for `HasProperty(dog, fur)` directly - not found.
    2.  Find rule: `If IsA(X, Y) and HasProperty(Y, Z), then HasProperty(X, Z)`.
    3.  Match `X=dog`, `Y=mammal` with `IsA(dog, mammal)`.
    4.  Match `Y=mammal`, `Z=fur` with `HasProperty(mammal, fur)`.
    5.  Infer `HasProperty(dog, fur)`. Output "Yes".

Case Study:
Medical diagnosis expert systems are classic examples of knowledge representation and reasoning in action. These systems encode extensive medical knowledge about diseases, symptoms, treatments, and their relationships as rules and facts. When a patient presents with symptoms, the system uses inference mechanisms (often backward chaining from possible diagnoses or forward chaining from symptoms) to deduce the most likely diagnosis and recommend treatments, all based on the explicitly represented medical knowledge.

Exercise:
Design a small knowledge base for a miniature expert system to recommend a simple movie genre to a user based on their mood and time of day.
*   Define at least three facts (e.g., `Mood(happy)`, `TimeOfDay(evening)`).
*   Define two simple "if-then" rules (e.g., `IF Mood(happy) AND TimeOfDay(evening) THEN Recommend(comedy)`).
*   Manually trace how the system would deduce a recommendation based on your facts and rules.

Tip/Pitfall:
A common pitfall is representing knowledge in an overly rigid way that struggles with exceptions or uncertainty. For instance, a rule like "All birds fly" is too strict and would lead to incorrect conclusions for penguins. Real-world KR often requires techniques like default logic or probabilistic reasoning to handle common-sense exceptions and varying degrees of belief, rather than absolute truths.